{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Evaluation Metrics Module for TextVQA\n","Includes: Accuracy, BLEU, METEOR, ROUGE, F1 Score,\n","          Precision/Recall (substring), LLM-as-Judge, Per-Category Performance\n","\"\"\"\n","\n","import re\n","from typing import List, Dict, Tuple, Optional\n","from collections import Counter\n","import numpy as np"],"metadata":{"id":"Dsid6GxanL_S","executionInfo":{"status":"ok","timestamp":1765595651575,"user_tz":-480,"elapsed":11,"user":{"displayName":"Lavinia Xu","userId":"05612180516970158833"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"HUNfZ6AvnJjV","executionInfo":{"status":"ok","timestamp":1765595689898,"user_tz":-480,"elapsed":127,"user":{"displayName":"Lavinia Xu","userId":"05612180516970158833"}}},"outputs":[],"source":["def normalize_answer(answer: str) -> str:\n","    \"\"\"\n","    Normalize answer string for comparison\n","    - Lowercase\n","    - Remove punctuation\n","    - Remove extra whitespace\n","    \"\"\"\n","    answer = answer.lower().strip()\n","    # Remove punctuation\n","    answer = re.sub(r'[^\\w\\s]', '', answer)\n","    # Remove extra whitespace\n","    answer = ' '.join(answer.split())\n","    return answer\n","\n","\n","def textvqa_accuracy(prediction: str, ground_truths: List[str]) -> float:\n","    \"\"\"\n","    Calculate TextVQA accuracy for a single prediction\n","    Uses exact string matching with normalization\n","    Following TextVQA evaluation protocol:\n","    acc = min(1, #humans that provided that answer / 3)\n","\n","    Args:\n","        prediction: Model prediction\n","        ground_truths: List of human answers (typically 10 annotators)\n","\n","    Returns:\n","        Accuracy score between 0 and 1\n","    \"\"\"\n","    if ground_truths is None or len(ground_truths) == 0:\n","        return 0.0\n","\n","    pred_normalized = normalize_answer(prediction)\n","    gt_normalized = [normalize_answer(gt) for gt in ground_truths]\n","\n","    # Count how many annotators gave the same answer as prediction\n","    match_count = sum(1 for gt in gt_normalized if gt == pred_normalized)\n","\n","    # TextVQA accuracy formula\n","    return min(1.0, match_count / 3.0)\n","\n","\n","def compute_accuracy(predictions: List[str], ground_truths: List[List[str]]) -> Dict[str, float]:\n","    \"\"\"\n","    Compute accuracy metrics over a batch of predictions\n","\n","    Args:\n","        predictions: List of model predictions\n","        ground_truths: List of answer lists (one list per question)\n","\n","    Returns:\n","        Dictionary with accuracy metrics\n","    \"\"\"\n","    accuracies = []\n","    exact_matches = []\n","\n","    for pred, gts in zip(predictions, ground_truths):\n","        acc = textvqa_accuracy(pred, gts)\n","        accuracies.append(acc)\n","\n","        # Also compute exact match (any annotator)\n","        pred_norm = normalize_answer(pred)\n","        gt_norms = [normalize_answer(gt) for gt in gts]\n","        exact_matches.append(1.0 if pred_norm in gt_norms else 0.0)\n","\n","    return {\n","        \"accuracy\": np.mean(accuracies),\n","        \"exact_match\": np.mean(exact_matches),\n","        \"num_samples\": len(predictions),\n","    }\n","\n","\n","def compute_bleu(prediction: str, references: List[str], n: int = 4) -> float:\n","    \"\"\"\n","    Compute BLEU score for a single prediction\n","\n","    Args:\n","        prediction: Model prediction\n","        references: Reference answers\n","        n: Maximum n-gram order\n","\n","    Returns:\n","        BLEU score\n","    \"\"\"\n","    from collections import Counter\n","    import math\n","\n","    def get_ngrams(text: str, n: int) -> Counter:\n","        words = text.split()\n","        return Counter(tuple(words[i:i+n]) for i in range(max(0, len(words) - n + 1)))\n","\n","    pred_words = normalize_answer(prediction).split()\n","    if not pred_words:\n","        return 0.0\n","\n","    # Compute n-gram precisions\n","    precisions = []\n","    for i in range(1, n + 1):\n","        pred_ngrams = get_ngrams(normalize_answer(prediction), i)\n","        if not pred_ngrams:\n","            precisions.append(0.0)\n","            continue\n","\n","        max_counts = Counter()\n","        for ref in references:\n","            ref_ngrams = get_ngrams(normalize_answer(ref), i)\n","            for ngram in pred_ngrams:\n","                max_counts[ngram] = max(max_counts[ngram], ref_ngrams[ngram])\n","\n","        clipped_count = sum(min(count, max_counts[ngram]) for ngram, count in pred_ngrams.items())\n","        total_count = sum(pred_ngrams.values())\n","        precisions.append(clipped_count / total_count if total_count > 0 else 0.0)\n","\n","    # Compute brevity penalty\n","    pred_len = len(pred_words)\n","    ref_lens = [len(normalize_answer(ref).split()) for ref in references]\n","    closest_ref_len = min(ref_lens, key=lambda x: (abs(x - pred_len), x)) if ref_lens else 0\n","\n","    if pred_len == 0:\n","        bp = 0\n","    elif pred_len >= closest_ref_len:\n","        bp = 1.0\n","    else:\n","        bp = math.exp(1 - closest_ref_len / pred_len)\n","\n","    # Compute BLEU score\n","    if min(precisions) > 0:\n","        log_precisions = [math.log(p) for p in precisions]\n","        bleu = bp * math.exp(sum(log_precisions) / len(log_precisions))\n","    else:\n","        bleu = 0.0\n","\n","    return bleu\n","\n","\n","def compute_rouge_l(prediction: str, reference: str) -> Dict[str, float]:\n","    \"\"\"\n","    Compute ROUGE-L score based on longest common subsequence\n","\n","    Args:\n","        prediction: Model prediction\n","        reference: Reference answer\n","\n","    Returns:\n","        Dictionary with precision, recall, and F1\n","    \"\"\"\n","    def lcs_length(x: List[str], y: List[str]) -> int:\n","        m, n = len(x), len(y)\n","        dp = [[0] * (n + 1) for _ in range(m + 1)]\n","        for i in range(1, m + 1):\n","            for j in range(1, n + 1):\n","                if x[i-1] == y[j-1]:\n","                    dp[i][j] = dp[i-1][j-1] + 1\n","                else:\n","                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n","        return dp[m][n]\n","\n","    pred_tokens = normalize_answer(prediction).split()\n","    ref_tokens = normalize_answer(reference).split()\n","\n","    if not pred_tokens or not ref_tokens:\n","        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n","\n","    lcs = lcs_length(pred_tokens, ref_tokens)\n","\n","    precision = lcs / len(pred_tokens)\n","    recall = lcs / len(ref_tokens)\n","    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n","\n","    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","\n","def compute_meteor(prediction: str, references: List[str]) -> float:\n","    \"\"\"\n","    Simplified METEOR score computation\n","    Based on unigram precision and recall with harmonic mean\n","\n","    Args:\n","        prediction: Model prediction\n","        references: Reference answers\n","\n","    Returns:\n","        METEOR score\n","    \"\"\"\n","    pred_tokens = set(normalize_answer(prediction).split())\n","    if not pred_tokens:\n","        return 0.0\n","\n","    best_score = 0.0\n","    for ref in references:\n","        ref_tokens = set(normalize_answer(ref).split())\n","        if not ref_tokens:\n","            continue\n","\n","        matches = len(pred_tokens & ref_tokens)\n","        precision = matches / len(pred_tokens) if pred_tokens else 0\n","        recall = matches / len(ref_tokens) if ref_tokens else 0\n","\n","        if precision + recall > 0:\n","            # METEOR uses weighted harmonic mean (recall weighted higher)\n","            alpha = 0.9  # Weight for recall\n","            score = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n","            best_score = max(best_score, score)\n","\n","    return best_score\n","\n","\n","def compute_f1_score(prediction: str, ground_truths: List[str]) -> float:\n","    \"\"\"\n","    Compute token-level F1 score\n","\n","    Args:\n","        prediction: Model prediction\n","        ground_truths: Reference answers\n","\n","    Returns:\n","        Maximum F1 score across all references\n","    \"\"\"\n","    pred_tokens = normalize_answer(prediction).split()\n","    if not pred_tokens:\n","        return 0.0\n","\n","    best_f1 = 0.0\n","    for gt in ground_truths:\n","        gt_tokens = normalize_answer(gt).split()\n","        if not gt_tokens:\n","            continue\n","\n","        common = Counter(pred_tokens) & Counter(gt_tokens)\n","        num_common = sum(common.values())\n","\n","        if num_common == 0:\n","            continue\n","\n","        precision = num_common / len(pred_tokens)\n","        recall = num_common / len(gt_tokens)\n","        f1 = 2 * precision * recall / (precision + recall)\n","        best_f1 = max(best_f1, f1)\n","\n","    return best_f1\n","\n","\n","def compute_substring_metrics(prediction: str, ground_truths: List[str]) -> Dict[str, float]:\n","    \"\"\"\n","    Compute Precision/Recall based on substring matching\n","\n","    Precision: Is the prediction contained in any ground truth?\n","    Recall: Is any ground truth contained in the prediction?\n","\n","    Args:\n","        prediction: Model prediction\n","        ground_truths: Reference answers\n","\n","    Returns:\n","        Dictionary with precision, recall, and F1\n","    \"\"\"\n","    pred_norm = normalize_answer(prediction)\n","    if not pred_norm:\n","        return {\"substring_precision\": 0.0, \"substring_recall\": 0.0, \"substring_f1\": 0.0}\n","\n","    gt_norms = [normalize_answer(gt) for gt in ground_truths if normalize_answer(gt)]\n","    if not gt_norms:\n","        return {\"substring_precision\": 0.0, \"substring_recall\": 0.0, \"substring_f1\": 0.0}\n","\n","    # Precision: prediction is substring of any ground truth\n","    precision = 1.0 if any(pred_norm in gt for gt in gt_norms) else 0.0\n","\n","    # Recall: any ground truth is substring of prediction\n","    recall = 1.0 if any(gt in pred_norm for gt in gt_norms) else 0.0\n","\n","    # F1\n","    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n","\n","    return {\n","        \"substring_precision\": precision,\n","        \"substring_recall\": recall,\n","        \"substring_f1\": f1\n","    }\n","\n","\n","def classify_question_type(question: str) -> str:\n","    \"\"\"\n","    Classify question into categories based on keywords\n","\n","    Categories:\n","    - brand: Questions about brand names\n","    - number: Questions about numbers, counts, dates\n","    - text: Questions about reading specific text\n","    - color: Questions about colors\n","    - time: Questions about time\n","    - person: Questions about people/names\n","    - location: Questions about places\n","    - yes_no: Yes/No questions\n","    - other: Everything else\n","    \"\"\"\n","    question_lower = question.lower()\n","\n","    # Yes/No questions\n","    if question_lower.startswith(('is ', 'are ', 'was ', 'were ', 'does ', 'do ', 'did ', 'can ', 'could ')):\n","        return \"yes_no\"\n","\n","    # Brand questions\n","    if any(kw in question_lower for kw in ['brand', 'company', 'manufacturer', 'make of', 'made by']):\n","        return \"brand\"\n","\n","    # Number questions\n","    if any(kw in question_lower for kw in ['how many', 'how much', 'number', 'count', 'price', 'cost', 'year', 'date']):\n","        return \"number\"\n","\n","    # Time questions\n","    if any(kw in question_lower for kw in ['time', 'when', 'clock', 'hour']):\n","        return \"time\"\n","\n","    # Color questions\n","    if any(kw in question_lower for kw in ['color', 'colour']):\n","        return \"color\"\n","\n","    # Person questions\n","    if any(kw in question_lower for kw in ['who', 'name of the person', 'author', 'writer', 'player']):\n","        return \"person\"\n","\n","    # Location questions\n","    if any(kw in question_lower for kw in ['where', 'location', 'place', 'state', 'country', 'city']):\n","        return \"location\"\n","\n","    # Text reading questions\n","    if any(kw in question_lower for kw in ['what does', 'what is written', 'say', 'read', 'spell', 'text']):\n","        return \"text\"\n","\n","    return \"other\"\n","\n","\n","def compute_per_category_metrics(\n","    predictions: List[str],\n","    ground_truths: List[List[str]],\n","    questions: List[str]\n",") -> Dict[str, Dict[str, float]]:\n","    \"\"\"\n","    Compute metrics broken down by question category\n","\n","    Args:\n","        predictions: List of model predictions\n","        ground_truths: List of answer lists\n","        questions: List of question strings\n","\n","    Returns:\n","        Dictionary mapping category to metrics\n","    \"\"\"\n","    # Group by category\n","    category_data = {}\n","    for pred, gts, q in zip(predictions, ground_truths, questions):\n","        cat = classify_question_type(q)\n","        if cat not in category_data:\n","            category_data[cat] = {\"predictions\": [], \"ground_truths\": []}\n","        category_data[cat][\"predictions\"].append(pred)\n","        category_data[cat][\"ground_truths\"].append(gts)\n","\n","    # Compute metrics per category\n","    category_metrics = {}\n","    for cat, data in category_data.items():\n","        acc_metrics = compute_accuracy(data[\"predictions\"], data[\"ground_truths\"])\n","        category_metrics[cat] = {\n","            \"count\": len(data[\"predictions\"]),\n","            \"accuracy\": acc_metrics[\"accuracy\"],\n","            \"exact_match\": acc_metrics[\"exact_match\"]\n","        }\n","\n","    return category_metrics\n","\n","\n","def llm_similarity_score(\n","    prediction: str,\n","    ground_truths: List[str],\n","    model=None,\n","    processor=None\n",") -> float:\n","    \"\"\"\n","    Use LLM to judge semantic similarity between prediction and ground truths\n","\n","    Args:\n","        prediction: Model prediction\n","        ground_truths: Reference answers\n","        model: VLM model (optional, uses simple heuristic if None)\n","        processor: Model processor (optional)\n","\n","    Returns:\n","        Similarity score between 0 and 1\n","    \"\"\"\n","    import torch\n","\n","    gt_sample = ground_truths[0] if ground_truths else \"\"\n","    prompt = f\"\"\"Judge if these two answers are semantically equivalent for a visual question answering task.\n","    Answer 1: {prediction}\n","    Answer 2: {gt_sample}\n","    Reply with only 'yes' or 'no'.\"\"\"\n","\n","    conversation = [{\"role\": \"user\", \"content\": prompt}]\n","    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n","    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","      output_ids = model.generate(**inputs, max_new_tokens=10)\n","      input_len = inputs[\"input_ids\"].shape[1]\n","      response = processor.batch_decode(output_ids[:, input_len:], skip_special_tokens=True)[0].strip().lower()\n","\n","    return 1.0 if 'yes' in response else 0.0"]},{"cell_type":"code","source":["def compute_all_metrics(\n","    predictions: List[str],\n","    ground_truths: List[List[str]],\n","    questions: Optional[List[str]] = None,\n","    model=None,\n","    processor=None,\n","    use_llm_judge: bool = False\n",") -> Dict[str, float]:\n","    \"\"\"\n","    Compute all evaluation metrics\n","\n","    Args:\n","        predictions: List of model predictions\n","        ground_truths: List of answer lists\n","        questions: Optional list of questions for per-category metrics\n","        model: Optional model for LLM-as-judge\n","        processor: Optional processor for LLM-as-judge\n","        use_llm_judge: Whether to use LLM for similarity scoring\n","\n","    Returns:\n","        Dictionary with all metrics\n","    \"\"\"\n","    # Accuracy metrics\n","    accuracy_metrics = compute_accuracy(predictions, ground_truths)\n","\n","    # Other metrics\n","    bleu_scores = []\n","    meteor_scores = []\n","    rouge_l_f1_scores = []\n","    f1_scores = []\n","    substring_precisions = []\n","    substring_recalls = []\n","    llm_similarities = []\n","\n","    for pred, gts in zip(predictions, ground_truths):\n","        bleu_scores.append(compute_bleu(pred, gts))\n","        meteor_scores.append(compute_meteor(pred, gts))\n","        f1_scores.append(compute_f1_score(pred, gts))\n","\n","        # ROUGE-L against best matching reference\n","        best_rouge = 0.0\n","        for gt in gts:\n","            rouge = compute_rouge_l(pred, gt)\n","            best_rouge = max(best_rouge, rouge[\"f1\"])\n","        rouge_l_f1_scores.append(best_rouge)\n","\n","        # Substring metrics\n","        substr_metrics = compute_substring_metrics(pred, gts)\n","        substring_precisions.append(substr_metrics[\"substring_precision\"])\n","        substring_recalls.append(substr_metrics[\"substring_recall\"])\n","\n","        # LLM similarity\n","        llm_similarities.append(llm_similarity_score(pred, gts, model, processor))\n","\n","    metrics = {\n","        \"accuracy\": accuracy_metrics[\"accuracy\"],\n","        \"exact_match\": accuracy_metrics[\"exact_match\"],\n","        \"bleu\": np.mean(bleu_scores),\n","        \"meteor\": np.mean(meteor_scores),\n","        \"rouge_l\": np.mean(rouge_l_f1_scores),\n","        \"f1\": np.mean(f1_scores),\n","        \"substring_precision\": np.mean(substring_precisions),\n","        \"substring_recall\": np.mean(substring_recalls),\n","        \"llm_similarity\": np.mean(llm_similarities),\n","        \"num_samples\": len(predictions),\n","    }\n","\n","    # Add per-category metrics if questions provided\n","    if questions is not None:\n","        metrics[\"per_category\"] = compute_per_category_metrics(predictions, ground_truths, questions)\n","\n","    return metrics\n","\n","\n","def print_metrics(metrics: Dict[str, float], title: str = \"Evaluation Results\"):\n","    \"\"\"Pretty print evaluation metrics\"\"\"\n","    print(f\"\\n{'='*50}\")\n","    print(f\"{title}\")\n","    print(f\"{'='*50}\")\n","    print(f\"Number of samples: {metrics.get('num_samples', 'N/A')}\")\n","    print(f\"-\"*50)\n","    print(f\"Accuracy (Primary):     {metrics.get('accuracy', 0.0)*100:.2f}%\")\n","    print(f\"Exact Match:            {metrics.get('exact_match', 0.0)*100:.2f}%\")\n","    print(f\"-\"*50)\n","    print(f\"BLEU:                   {metrics.get('bleu', 0.0)*100:.2f}\")\n","    print(f\"METEOR:                 {metrics.get('meteor', 0.0)*100:.2f}\")\n","    print(f\"ROUGE-L:                {metrics.get('rouge_l', 0.0)*100:.2f}\")\n","    print(f\"F1 Score:               {metrics.get('f1', 0.0)*100:.2f}\")\n","    print(f\"-\"*50)\n","    print(f\"Substring Precision:    {metrics.get('substring_precision', 0.0)*100:.2f}%\")\n","    print(f\"Substring Recall:       {metrics.get('substring_recall', 0.0)*100:.2f}%\")\n","    print(f\"llm Similarity:    {metrics.get('llm_similarity', 0.0)*100:.2f}%\")\n","\n","    # Print per-category metrics if available\n","    if \"per_category\" in metrics:\n","        print(f\"\\n{'-'*50}\")\n","        print(\"Per-Category Performance:\")\n","        print(f\"{'-'*50}\")\n","        for cat, cat_metrics in sorted(metrics[\"per_category\"].items(), key=lambda x: -x[1][\"count\"]):\n","            print(f\"  {cat:12s}: {cat_metrics['accuracy']*100:5.1f}% acc, \"\n","                  f\"{cat_metrics['exact_match']*100:5.1f}% EM (n={cat_metrics['count']})\")\n","\n","    print(f\"{'='*50}\\n\")\n","\n","\n","if __name__ == \"__main__\":\n","    # Test metrics\n","    predictions = [\"nokia\", \"samsung\", \"apple iphone\"]\n","    ground_truths = [\n","        [\"nokia\", \"nokia\", \"nokia\", \"toshiba\"],\n","        [\"samsung\", \"samsung galaxy\", \"samsung\"],\n","        [\"iphone\", \"apple\", \"iphone x\"],\n","    ]\n","\n","    metrics = compute_all_metrics(predictions, ground_truths)\n","    print_metrics(metrics, \"Test Metrics\")"],"metadata":{"id":"9jHC56dQnX7z","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"error","timestamp":1765595700788,"user_tz":-480,"elapsed":3682,"user":{"displayName":"Lavinia Xu","userId":"05612180516970158833"}},"outputId":"ea81f2f6-0257-4688-aec0-0567fa5bc93b"},"execution_count":3,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'NoneType' object has no attribute 'apply_chat_template'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3248621421.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m     ]\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_all_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mprint_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test Metrics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3248621421.py\u001b[0m in \u001b[0;36mcompute_all_metrics\u001b[0;34m(predictions, ground_truths, questions, model, processor, use_llm_judge)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# LLM similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mllm_similarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_similarity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     metrics = {\n","\u001b[0;32m/tmp/ipython-input-3309170531.py\u001b[0m in \u001b[0;36mllm_similarity_score\u001b[0;34m(prediction, ground_truths, model, processor)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mconversation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'apply_chat_template'"]}]}]}