{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1SXKAH6Z-l3-DEx0EpD3w2eWX9QXRT7kh","authorship_tag":"ABX9TyOGRkvyG51ZmlGDy2nX4HmP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install import-ipynb"],"metadata":{"id":"T5GmrShm7lGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/SHBT261FinalProject/code/src"],"metadata":{"id":"fhLLHq-U7mgY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","LoRA Fine-tuning Script for TextVQA\n","Fine-tunes Qwen2.5-VL with LoRA adapters on TextVQA training set\n","\"\"\"\n","import os\n","import json\n","from datetime import datetime\n","from collections import Counter\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import (\n","    get_linear_schedule_with_warmup,\n","    get_cosine_schedule_with_warmup,\n",")\n","from tqdm import tqdm\n","from PIL import Image\n","\n","import import_ipynb\n","from data_loader import TextVQADataset\n","from model import get_model_and_processor, save_lora_weights, get_generation_config\n","from metrics import compute_all_metrics, print_metrics"],"metadata":{"id":"CfIXEgFOuijP","executionInfo":{"status":"ok","timestamp":1765312235226,"user_tz":300,"elapsed":35917,"user":{"displayName":"Rita Fan","userId":"10829651473634783011"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class TrainingConfig:\n","    def __init__(\n","        self,\n","        model_name=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n","        data_dir=\"textvqa_data/data\",\n","        output_dir=\"checkpoints\",\n","        lora_r=32,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        num_epochs=3,\n","        batch_size=2,\n","        gradient_accumulation_steps=8,\n","        learning_rate=2e-4,\n","        warmup_ratio=0.1,\n","        weight_decay=0.01,\n","        max_grad_norm=1.0,\n","        max_train_samples=None,\n","        max_val_samples=500,\n","        use_4bit=True,\n","        seed=42,\n","        eval_steps=500,\n","        save_steps=500,\n","        logging_steps=50,\n","    ):\n","        self.model_name = model_name\n","        self.data_dir = data_dir\n","        self.output_dir = output_dir\n","        self.lora_r = lora_r\n","        self.lora_alpha = lora_alpha\n","        self.lora_dropout = lora_dropout\n","        self.num_epochs = num_epochs\n","        self.batch_size = batch_size\n","        self.gradient_accumulation_steps = gradient_accumulation_steps\n","        self.learning_rate = learning_rate\n","        self.warmup_ratio = warmup_ratio\n","        self.weight_decay = weight_decay\n","        self.max_grad_norm = max_grad_norm\n","        self.max_train_samples = max_train_samples\n","        self.max_val_samples = max_val_samples\n","        self.use_4bit = use_4bit\n","        self.seed = seed\n","        self.eval_steps = eval_steps\n","        self.save_steps = save_steps\n","        self.logging_steps = logging_steps"],"metadata":{"id":"zNSupf7PTH5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_most_common_answer(answers):\n","    if not answers:\n","        return \"\"\n","    counter = Counter(answers)\n","    return counter.most_common(1)[0][0]\n","\n","def prepare_training_batch(batch, processor, device):\n","    images, questions, answers = [], [], []\n","\n","    for sample in batch:\n","        images.append(sample[\"image\"])\n","        questions.append(sample[\"question\"])\n","        answers.append(get_most_common_answer(sample[\"answers\"]))\n","\n","    conversations = []\n","    for q, a in zip(questions, answers):\n","        conv = [\n","            {\"role\": \"user\",\n","             \"content\": [\n","                 {\"type\": \"image\"},\n","                 {\"type\": \"text\",\n","                  \"text\": f\"{q}\\nAnswer with only the exact text/number from the image.\"}\n","             ]},\n","            {\"role\": \"assistant\", \"content\": a},\n","        ]\n","        conversations.append(conv)\n","\n","    texts = [\n","        processor.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)\n","        for conv in conversations\n","    ]\n","\n","    inputs = processor(\n","        text=texts, images=images, padding=True, return_tensors=\"pt\"\n","    )\n","    inputs = {k: v.to(device) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n","    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n","\n","    return inputs"],"metadata":{"id":"xB9BUnpVTKbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, processor, dataset, max_samples=None, device=\"cuda\"):\n","    model.eval()\n","    predictions, ground_truths = [], []\n","    gen_cfg = get_generation_config()\n","\n","    n = min(len(dataset), max_samples or len(dataset))\n","\n","    with torch.no_grad():\n","        for idx in tqdm(range(n), desc=\"Evaluating\", leave=False):\n","            sample = dataset[idx]\n","            img = sample[\"image\"]\n","            q = sample[\"question\"]\n","\n","            conv = [\n","                {\"role\": \"user\",\n","                 \"content\": [\n","                     {\"type\": \"image\"},\n","                     {\"type\": \"text\", \"text\": f\"{q}\\nAnswer concisely.\"}\n","                 ]}\n","            ]\n","\n","            text = processor.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n","            inputs = processor(text=[text], images=[img], return_tensors=\"pt\", padding=True)\n","            inputs = {k: v.to(device) if hasattr(v, \"to\") else v for k, v in inputs.items()}\n","\n","            out_ids = model.generate(**inputs, **gen_cfg)\n","            input_len = inputs[\"input_ids\"].shape[1]\n","            gen_ids = out_ids[:, input_len:]\n","\n","            pred = processor.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n","            predictions.append(pred)\n","            ground_truths.append(sample[\"answers\"])\n","\n","    metrics = compute_all_metrics(predictions, ground_truths)\n","    model.train()\n","    return metrics"],"metadata":{"id":"vJHgEHO5TNKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(config: TrainingConfig):\n","    print(\"\\n==============================\")\n","    print(\"   TextVQA LoRA Fine-tuning   \")\n","    print(\"==============================\")\n","\n","    torch.manual_seed(config.seed)\n","\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    run_dir = os.path.join(config.output_dir, f\"run_{timestamp}\")\n","    os.makedirs(run_dir, exist_ok=True)\n","\n","    with open(os.path.join(run_dir, \"config.json\"), \"w\") as f:\n","        json.dump(vars(config), f, indent=2)\n","\n","    print(\"Loading model...\")\n","    model, processor = get_model_and_processor(\n","        model_name=config.model_name,\n","        use_4bit=config.use_4bit,\n","        use_lora=True,\n","        lora_r=config.lora_r,\n","        lora_alpha=config.lora_alpha,\n","        lora_dropout=config.lora_dropout,\n","    )\n","    device = next(model.parameters()).device\n","    print(\"Model loaded on:\", device)\n","\n","    print(\"Loading datasets...\")\n","    train_ds = TextVQADataset(config.data_dir, \"train\", max_samples=config.max_train_samples)\n","    val_ds = TextVQADataset(config.data_dir, \"validation\", max_samples=config.max_val_samples)\n","\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=config.batch_size,\n","        shuffle=True,\n","        num_workers=0,     # IMPORTANT for Colab\n","        collate_fn=lambda x: x,\n","    )\n","\n","    total_steps = (len(train_loader) // config.gradient_accumulation_steps) * config.num_epochs\n","    warmup_steps = int(total_steps * config.warmup_ratio)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","\n","    model.train()\n","    global_step, best_acc = 0, 0.0\n","    log = []\n","\n","    for epoch in range(config.num_epochs):\n","        epoch_loss = 0\n","        print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n","\n","        for step, batch in enumerate(tqdm(train_loader)):\n","            inputs = prepare_training_batch(batch, processor, device)\n","            outputs = model(**inputs)\n","            loss = outputs.loss / config.gradient_accumulation_steps\n","            loss.backward()\n","\n","            epoch_loss += loss.item() * config.gradient_accumulation_steps\n","\n","            if (step + 1) % config.gradient_accumulation_steps == 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n","                optimizer.step()\n","                scheduler.step()\n","                optimizer.zero_grad()\n","                global_step += 1\n","\n","                if global_step % config.logging_steps == 0:\n","                    avg_loss = epoch_loss / (step + 1)\n","                    log.append({\"step\": global_step, \"loss\": avg_loss})\n","\n","                if global_step % config.eval_steps == 0:\n","                    metrics = evaluate_model(model, processor, val_ds, device=device)\n","                    print_metrics(metrics, f\"Validation @ step {global_step}\")\n","                    if metrics[\"accuracy\"] > best_acc:\n","                        best_acc = metrics[\"accuracy\"]\n","                        save_lora_weights(model, os.path.join(run_dir, \"best_model\"))\n","\n","        print(f\"Epoch {epoch+1} finished. Loss: {epoch_loss:.4f}\")\n","\n","    save_lora_weights(model, os.path.join(run_dir, \"final_model\"))\n","    print(\"Training completed.\")"],"metadata":{"id":"sazlQVGxTOjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5tGGUYXSSbDt"},"execution_count":null,"outputs":[]}]}