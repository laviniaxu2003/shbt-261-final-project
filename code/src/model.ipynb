{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlUZzGEGlPpt1ufXxuvw/2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Model Configuration and Initialization Module\n","Supports Qwen2.5-VL-3B with LoRA fine-tuning\n","\"\"\"\n","\n","import torch\n","from transformers import (\n","    Qwen2_5_VLForConditionalGeneration,\n","    AutoProcessor,\n","    BitsAndBytesConfig,\n",")\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_kbit_training,\n","    TaskType,\n",")\n","from typing import Optional, Tuple"],"metadata":{"id":"Bwar0tNopRk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBjJlL_lpMz7"},"outputs":[],"source":["def get_model_and_processor(\n","    model_name: str = \"Qwen/Qwen2.5-VL-3B-Instruct\",\n","    use_4bit: bool = True,\n","    use_lora: bool = False,\n","    lora_r: int = 16,\n","    lora_alpha: int = 32,\n","    lora_dropout: float = 0.05,\n","    device_map: str = \"auto\",\n",") -> Tuple:\n","    \"\"\"\n","    Load the Qwen2.5-VL model and processor\n","\n","    Args:\n","        model_name: HuggingFace model name\n","        use_4bit: Whether to use 4-bit quantization\n","        use_lora: Whether to apply LoRA adapters\n","        lora_r: LoRA rank\n","        lora_alpha: LoRA alpha\n","        lora_dropout: LoRA dropout\n","        device_map: Device mapping strategy\n","\n","    Returns:\n","        model, processor tuple\n","    \"\"\"\n","    print(f\"Loading model: {model_name}\")\n","\n","    # Load processor\n","    processor = AutoProcessor.from_pretrained(\n","        model_name,\n","        trust_remote_code=True,\n","    )\n","\n","    # Configure quantization\n","    if use_4bit:\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","    else:\n","        bnb_config = None\n","\n","    # Load model\n","    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n","        model_name,\n","        quantization_config=bnb_config,\n","        device_map=device_map,\n","        trust_remote_code=True,\n","        torch_dtype=torch.bfloat16 if not use_4bit else None,\n","    )\n","\n","    # Apply LoRA if requested\n","    if use_lora:\n","        model = apply_lora(\n","            model,\n","            r=lora_r,\n","            alpha=lora_alpha,\n","            dropout=lora_dropout,\n","            use_4bit=use_4bit,\n","        )\n","\n","    return model, processor\n","\n","\n","def apply_lora(\n","    model,\n","    r: int = 16,\n","    alpha: int = 32,\n","    dropout: float = 0.05,\n","    use_4bit: bool = True,\n",") -> torch.nn.Module:\n","    \"\"\"\n","    Apply LoRA adapters to the model\n","\n","    Args:\n","        model: The base model\n","        r: LoRA rank\n","        alpha: LoRA alpha scaling factor\n","        dropout: Dropout probability\n","        use_4bit: Whether model is quantized\n","\n","    Returns:\n","        Model with LoRA adapters\n","    \"\"\"\n","    print(\"Applying LoRA adapters...\")\n","\n","    # Prepare model for k-bit training if quantized\n","    if use_4bit:\n","        model = prepare_model_for_kbit_training(model)\n","\n","    # Define target modules for Qwen2.5-VL\n","    # Target the attention and MLP layers in the language model\n","    target_modules = [\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        \"gate_proj\",\n","        \"up_proj\",\n","        \"down_proj\",\n","    ]\n","\n","    # LoRA configuration\n","    lora_config = LoraConfig(\n","        r=r,\n","        lora_alpha=alpha,\n","        lora_dropout=dropout,\n","        target_modules=target_modules,\n","        bias=\"none\",\n","        task_type=TaskType.CAUSAL_LM,\n","    )\n","\n","    # Apply LoRA\n","    model = get_peft_model(model, lora_config)\n","\n","    # Print trainable parameters\n","    model.print_trainable_parameters()\n","\n","    return model\n","\n","\n","def save_lora_weights(model, save_path: str):\n","    \"\"\"Save only the LoRA adapter weights\"\"\"\n","    print(f\"Saving LoRA weights to {save_path}\")\n","    model.save_pretrained(save_path)\n","\n","\n","def load_lora_weights(\n","    model_name: str = \"Qwen/Qwen2.5-VL-3B-Instruct\",\n","    lora_path: str = None,\n","    use_4bit: bool = True,\n","    device_map: str = \"auto\",\n",") -> Tuple:\n","    \"\"\"\n","    Load model with trained LoRA weights\n","\n","    Args:\n","        model_name: Base model name\n","        lora_path: Path to saved LoRA weights\n","        use_4bit: Whether to use 4-bit quantization\n","        device_map: Device mapping strategy\n","\n","    Returns:\n","        model, processor tuple\n","    \"\"\"\n","    from peft import PeftModel\n","\n","    # Load base model and processor\n","    model, processor = get_model_and_processor(\n","        model_name=model_name,\n","        use_4bit=use_4bit,\n","        use_lora=False,\n","        device_map=device_map,\n","    )\n","\n","    # Load LoRA weights\n","    if lora_path:\n","        print(f\"Loading LoRA weights from {lora_path}\")\n","        model = PeftModel.from_pretrained(model, lora_path)\n","\n","    return model, processor\n","\n","\n","def get_generation_config(\n","    max_new_tokens: int = 128,\n","    temperature: float = 0.1,\n","    top_p: float = 0.9,\n","    do_sample: bool = False,\n",") -> dict:\n","    \"\"\"Get generation configuration for inference\"\"\"\n","    return {\n","        \"max_new_tokens\": max_new_tokens,\n","        \"temperature\": temperature,\n","        \"top_p\": top_p,\n","        \"do_sample\": do_sample,\n","        \"pad_token_id\": 151643,  # Qwen pad token\n","    }"]}]}