{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1XPEuAuBqW0HGnkXjW31s2LX3oSZF8VBr","authorship_tag":"ABX9TyMSB9X/hQkjQr1kyAToW1yP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install import-ipynb"],"metadata":{"id":"G7TlEp3n9L8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/SHBT261FinalProject/code/src"],"metadata":{"id":"WkG9E6Mk9NE4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Zero-shot Evaluation Script for TextVQA\n","Evaluates pretrained VLM directly on validation/test sets\n","\"\"\"\n","\n","import os\n","import json\n","from tqdm import tqdm\n","from datetime import datetime\n","import torch\n","import numpy as np\n","\n","import import_ipynb\n","from data_loader import TextVQADataset\n","from model import get_model_and_processor, get_generation_config\n","from metrics import compute_all_metrics, print_metrics"],"metadata":{"id":"nQ903K1CmqTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_inference_zeroshot(\n","    model,\n","    processor,\n","    dataset,\n","    batch_size=1,\n","    max_samples=None,\n","    save_predictions=True,\n","    output_dir=\"results\",\n","    metric_filename=\"zeroshot_metric.json\",\n","    pred_filename=\"zeroshot_predictions.json\",\n","):\n","    model.eval()\n","\n","    predictions = []\n","    ground_truths = []\n","    questions_list = []\n","    results = []\n","\n","    num_samples = min(len(dataset), max_samples or len(dataset))\n","    gen_config = get_generation_config()\n","\n","    print(f\"\\nRunning inference on {num_samples} samples...\")\n","\n","    with torch.no_grad():\n","        for idx in tqdm(range(num_samples)):\n","            sample = dataset[idx]\n","            image = sample[\"image\"]\n","            question = sample[\"question\"]\n","            gt_answers = sample[\"answers\"]\n","\n","            # OCR-aware prompt\n","            ocr_tokens = sample.get(\"ocr_tokens\", [])\n","            ocr_str = \", \".join([str(t) for t in ocr_tokens]) if ocr_tokens else \"\"\n","            ocr_section = f\"OCR tokens detected: {ocr_str}.\\n\" if ocr_str else \"\"\n","\n","            text_prompt = (\n","                f\"{ocr_section}\"\n","                f\"Question: {question}\\n\"\n","                \"Please answer using one of the OCR tokens above or a short phrase derived from them.\"\n","            )\n","\n","            conv = [\n","                {\"role\": \"user\",\n","                 \"content\":[{\"type\": \"image\"}, {\"type\": \"text\",\"text\": text_prompt}]}\n","            ]\n","\n","            text = processor.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n","            inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True)\n","            device = next(model.parameters()).device\n","            inputs = {k: v.to(device) if hasattr(v, \"to\") else v for k,v in inputs.items()}\n","\n","            output_ids = model.generate(**inputs, **gen_config)\n","            input_len = inputs[\"input_ids\"].shape[1]\n","            pred_ids = output_ids[:, input_len:]\n","            prediction = processor.batch_decode(pred_ids, skip_special_tokens=True)[0].strip()\n","\n","            predictions.append(prediction)\n","            ground_truths.append(gt_answers)\n","            questions_list.append(question)\n","\n","            results.append({\n","                \"image_id\": str(sample[\"image_id\"]),\n","                \"question_id\": int(sample[\"question_id\"]),\n","                \"question\": question,\n","                \"prediction\": prediction,\n","                \"ground_truths\": gt_answers,\n","                \"ocr_tokens\": ocr_tokens,\n","            })\n","\n","    metrics = compute_all_metrics(predictions, ground_truths, questions=questions_list)\n","\n","    # save outputs\n","    if save_predictions:\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        pred_path = os.path.join(output_dir, pred_filename)\n","        with open(pred_path, \"w\") as f:\n","            json.dump(results, f, indent=2, ensure_ascii=False)\n","        print(\"Predictions saved:\", pred_path)\n","\n","        metric_path = os.path.join(output_dir, metric_filename)\n","        with open(metric_path, \"w\") as f:\n","            json.dump(metrics, f, indent=2)\n","        print(\"Metrics saved:\", metric_path)\n","\n","    return {\"predictions\": predictions, \"results\": results, \"metrics\": metrics}\n"],"metadata":{"id":"Whzx06dHoqwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BDJH-cB3oqt3"},"execution_count":null,"outputs":[]}]}