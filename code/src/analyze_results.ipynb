{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1LvX7DlbuAboUrDlE1aSJD0IHfSmv6TMb","authorship_tag":"ABX9TyO6Uam+UpnKmk/sCdqRJsjC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install import-ipynb"],"metadata":{"id":"nXP2OSf40Hia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/SHBT261FinalProject/code/src"],"metadata":{"id":"TRHXD1EI0Hd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Results Visualization and Error Analysis Module\n","Provides comprehensive analysis of model predictions\n","\"\"\"\n","\n","import os\n","import json\n","import re\n","from collections import Counter, defaultdict\n","from typing import List, Dict\n","\n","import import_ipynb\n","from metrics import normalize_answer, textvqa_accuracy"],"metadata":{"id":"viw8SuhylWM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_predictions(file_path: str) -> List[Dict]:\n","    \"\"\"Load predictions from JSON file\"\"\"\n","    with open(file_path, \"r\") as f:\n","        return json.load(f)\n","\n","\n","def categorize_error(prediction: str, ground_truths: List[str], ocr_tokens: List[str]) -> str:\n","    \"\"\"Categorize prediction error types\"\"\"\n","    pred_norm = normalize_answer(prediction)\n","    gt_norms = [normalize_answer(gt) for gt in ground_truths]\n","    ocr_norms = [normalize_answer(tok) for tok in ocr_tokens]\n","\n","    if pred_norm in gt_norms:\n","        return \"correct\"\n","\n","    for gt in gt_norms:\n","        if pred_norm in gt or gt in pred_norm:\n","            return \"format_error\"\n","\n","    gt_in_ocr = any(gt in \" \".join(ocr_norms) for gt in gt_norms)\n","    pred_in_ocr = pred_norm in \" \".join(ocr_norms) or any(pred_norm in tok for tok in ocr_norms)\n","\n","    if gt_in_ocr and not pred_in_ocr:\n","        return \"ocr_miss\"\n","    if pred_in_ocr and gt_in_ocr:\n","        return \"reasoning_error\"\n","    if not pred_in_ocr and ocr_tokens:\n","        return \"hallucination\"\n","\n","    return \"other\"\n","\n","\n","def analyze_errors(results: List[Dict]) -> Dict:\n","    \"\"\"Perform complete error analysis\"\"\"\n","    error_counts = Counter()\n","    error_examples = defaultdict(list)\n","    correct_count = 0\n","\n","    for r in results:\n","        pred = r[\"prediction\"]\n","        gts = r[\"ground_truths\"]\n","        ocr = r.get(\"ocr_tokens\", [])\n","\n","        acc = textvqa_accuracy(pred, gts)\n","        if acc > 0.5:\n","            correct_count += 1\n","            etype = \"correct\"\n","        else:\n","            etype = categorize_error(pred, gts, ocr)\n","\n","        error_counts[etype] += 1\n","        if len(error_examples[etype]) < 5:\n","            error_examples[etype].append({\n","                \"question\": r[\"question\"],\n","                \"prediction\": pred,\n","                \"ground_truths\": gts[:3],\n","                \"ocr_tokens\": ocr[:5]\n","            })\n","\n","    total = len(results)\n","    return {\n","        \"total_samples\": total,\n","        \"correct_count\": correct_count,\n","        \"accuracy\": correct_count / total if total else 0,\n","        \"error_distribution\": dict(error_counts),\n","        \"error_examples\": dict(error_examples),\n","    }\n","\n","\n","def analyze_question_types(results: List[Dict]) -> Dict:\n","    patterns = {\n","        \"what\": r\"^what\\s\",\n","        \"how many\": r\"^how many\\s\",\n","        \"what is the name\": r\"what is the name\",\n","        \"what is the number\": r\"what is the number|what number\",\n","        \"what is the brand\": r\"what (?:is the )?brand\",\n","        \"what is the time\": r\"what (?:is the )?time\",\n","        \"what is the date\": r\"what (?:is the )?date\",\n","        \"what is the price\": r\"what (?:is the )?price\",\n","        \"what color\": r\"what color\",\n","        \"which\": r\"^which\\s\",\n","        \"where\": r\"^where\\s\",\n","        \"who\": r\"^who\\s\",\n","    }\n","\n","    stats = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n","\n","    for r in results:\n","        q = r[\"question\"].lower()\n","        pred = r[\"prediction\"]\n","        gts = r[\"ground_truths\"]\n","\n","        correct = textvqa_accuracy(pred, gts) > 0.5\n","\n","        matched = False\n","        for key, pat in patterns.items():\n","            if re.search(pat, q):\n","                stats[key][\"total\"] += 1\n","                if correct:\n","                    stats[key][\"correct\"] += 1\n","                matched = True\n","                break\n","\n","        if not matched:\n","            stats[\"other\"][\"total\"] += 1\n","            if correct:\n","                stats[\"other\"][\"correct\"] += 1\n","\n","    for key in stats:\n","        t = stats[key][\"total\"]\n","        stats[key][\"accuracy\"] = stats[key][\"correct\"] / t if t else 0\n","\n","    return dict(stats)\n","\n","\n","def analyze_answer_length(results: List[Dict]) -> Dict:\n","    stats = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n","\n","    for r in results:\n","        gts = r[\"ground_truths\"]\n","        pred = r[\"prediction\"]\n","\n","        if gts:\n","            avg_len = sum(len(gt.split()) for gt in gts) / len(gts)\n","            if avg_len <= 1:\n","                cat = \"single_word\"\n","            elif avg_len <= 3:\n","                cat = \"short_phrase\"\n","            else:\n","                cat = \"long_phrase\"\n","        else:\n","            cat = \"unknown\"\n","\n","        correct = textvqa_accuracy(pred, gts) > 0.5\n","        stats[cat][\"total\"] += 1\n","        if correct:\n","            stats[cat][\"correct\"] += 1\n","\n","    for cat in stats:\n","        t = stats[cat][\"total\"]\n","        stats[cat][\"accuracy\"] = stats[cat][\"correct\"] / t if t else 0\n","\n","    return dict(stats)\n","\n","\n","def compare_models(zs_results: List[Dict], ft_results: List[Dict]) -> Dict:\n","    zs_map = {r[\"question_id\"]: r for r in zs_results}\n","    ft_map = {r[\"question_id\"]: r for r in ft_results}\n","\n","    common = set(zs_map) & set(ft_map)\n","\n","    comp = {\n","        \"zeroshot\": {\"correct\": 0, \"total\": len(common)},\n","        \"finetuned\": {\"correct\": 0, \"total\": len(common)},\n","        \"improvements\": [],\n","        \"regressions\": [],\n","    }\n","\n","    for qid in common:\n","        zs = zs_map[qid]\n","        ft = ft_map[qid]\n","\n","        zs_acc = textvqa_accuracy(zs[\"prediction\"], zs[\"ground_truths\"])\n","        ft_acc = textvqa_accuracy(ft[\"prediction\"], ft[\"ground_truths\"])\n","\n","        if zs_acc > 0.5: comp[\"zeroshot\"][\"correct\"] += 1\n","        if ft_acc > 0.5: comp[\"finetuned\"][\"correct\"] += 1\n","\n","        if ft_acc > zs_acc and len(comp[\"improvements\"]) < 10:\n","            comp[\"improvements\"].append({\n","                \"question\": zs[\"question\"],\n","                \"zeroshot_pred\": zs[\"prediction\"],\n","                \"finetuned_pred\": ft[\"prediction\"],\n","                \"ground_truths\": zs[\"ground_truths\"][:3],\n","            })\n","        elif ft_acc < zs_acc and len(comp[\"regressions\"]) < 10:\n","            comp[\"regressions\"].append({\n","                \"question\": zs[\"question\"],\n","                \"zeroshot_pred\": zs[\"prediction\"],\n","                \"finetuned_pred\": ft[\"prediction\"],\n","                \"ground_truths\": zs[\"ground_truths\"][:3],\n","            })\n","\n","    comp[\"zeroshot\"][\"accuracy\"] = comp[\"zeroshot\"][\"correct\"] / len(common)\n","    comp[\"finetuned\"][\"accuracy\"] = comp[\"finetuned\"][\"correct\"] / len(common)\n","    comp[\"improvement\"] = comp[\"finetuned\"][\"accuracy\"] - comp[\"zeroshot\"][\"accuracy\"]\n","    comp[\"num_compared\"] = len(common)\n","\n","    return comp\n"],"metadata":{"id":"4Ih0195A2x_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_analysis(predictions_path: str, output_dir=\"results\", model_name=\"Model\"):\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    results = load_predictions(predictions_path)\n","\n","    err = analyze_errors(results)\n","    qtype = analyze_question_types(results)\n","    length = analyze_answer_length(results)\n","\n","    out = {\n","        \"error_analysis\": err,\n","        \"question_type_analysis\": qtype,\n","        \"answer_length_analysis\": length\n","    }\n","\n","    out_json = os.path.join(output_dir, f\"analysis_detailed_{model_name}.json\")\n","    with open(out_json, \"w\") as f:\n","        json.dump(out, f, indent=2)\n","\n","    print(\"Saved:\", out_json)\n","    return out\n"],"metadata":{"id":"hDlLjsjG2yed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compare_two_models(zs_path, ft_path, output_dir=\"results\"):\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    zs = load_predictions(zs_path)\n","    ft = load_predictions(ft_path)\n","\n","    comp = compare_models(zs, ft)\n","\n","    out_path = os.path.join(output_dir, \"model_comparison.json\")\n","    with open(out_path, \"w\") as f:\n","        json.dump(comp, f, indent=2)\n","\n","    print(\"Saved comparison:\", out_path)\n","    return comp\n"],"metadata":{"id":"O9VghFnc20bs"},"execution_count":null,"outputs":[]}]}