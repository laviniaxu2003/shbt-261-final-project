{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwM/XyBCKRd++aSRAXQoE0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","TextVQA Dataset Loader and Preprocessing Module\n","\"\"\"\n","\n","import os\n","from typing import Dict, List, Optional, Tuple, Callable\n","from PIL import Image\n","import io\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","import torch"],"metadata":{"id":"sFtzas_9lyVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifKWMDEVlw3s"},"outputs":[],"source":["class TextVQADataset(Dataset):\n","    \"\"\"TextVQA Dataset for Visual Question Answering\"\"\"\n","\n","    def __init__(\n","        self,\n","        data_dir: str,\n","        split: str = \"train\",\n","        processor: Optional[Callable] = None,\n","        max_samples: Optional[int] = None,\n","    ):\n","        \"\"\"\n","        Args:\n","            data_dir: Path to the data directory containing parquet files\n","            split: One of 'train', 'validation', or 'test'\n","            processor: Model processor for preprocessing images and text\n","            max_samples: Maximum number of samples to load (for debugging)\n","        \"\"\"\n","        self.data_dir = data_dir\n","        self.split = split\n","        self.processor = processor\n","\n","        # Load data from parquet files\n","        self.data = self._load_parquet_files()\n","\n","        if max_samples is not None:\n","            self.data = self.data[:max_samples]\n","\n","        print(f\"Loaded {len(self.data)} samples for {split} split\")\n","\n","    def _load_parquet_files(self) -> pd.DataFrame:\n","        \"\"\"Load all parquet files for the specified split\"\"\"\n","        split_patterns = {\n","            \"train\": (\"train-\", 20),\n","            \"validation\": (\"validation-\", 3),\n","            \"test\": (\"test-\", 4),\n","        }\n","\n","        prefix, num_files = split_patterns[self.split]\n","        files = [\n","            os.path.join(self.data_dir, f\"{prefix}{str(i).zfill(5)}-of-{str(num_files).zfill(5)}.parquet\")\n","            for i in range(num_files)\n","        ]\n","\n","        dfs = []\n","        for f in files:\n","            if os.path.exists(f):\n","                dfs.append(pd.read_parquet(f))\n","\n","        return pd.concat(dfs, ignore_index=True)\n","\n","    def __len__(self) -> int:\n","        return len(self.data)\n","\n","    def __getitem__(self, idx: int) -> Dict:\n","        \"\"\"Get a single sample\"\"\"\n","        row = self.data.iloc[idx]\n","\n","        # Load image from bytes\n","        image_bytes = row[\"image\"][\"bytes\"]\n","        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n","\n","        # Get question and answers\n","        question = row[\"question\"]\n","        answers = row[\"answers\"] if \"answers\" in row and row[\"answers\"] is not None else []\n","        ocr_tokens = row[\"ocr_tokens\"] if \"ocr_tokens\" in row else []\n","\n","        sample = {\n","            \"image_id\": row[\"image_id\"],\n","            \"question_id\": row[\"question_id\"],\n","            \"image\": image,\n","            \"question\": question,\n","            \"answers\": answers,\n","            \"ocr_tokens\": ocr_tokens,\n","        }\n","\n","        return sample\n","\n","    def get_most_common_answer(self, answers: List[str]) -> str:\n","        \"\"\"Get the most common answer from the answer list\"\"\"\n","        if not answers:\n","            return \"\"\n","        from collections import Counter\n","        counter = Counter(answers)\n","        return counter.most_common(1)[0][0]\n","\n","\n","def collate_fn_qwen(batch: List[Dict], processor) -> Dict:\n","    \"\"\"Collate function for Qwen2.5-VL model\"\"\"\n","    images = [sample[\"image\"] for sample in batch]\n","    questions = [sample[\"question\"] for sample in batch]\n","\n","    # Create conversation format for Qwen2.5-VL\n","    conversations = []\n","    for q in questions:\n","        conv = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"image\"},\n","                    {\"type\": \"text\", \"text\": q}\n","                ]\n","            }\n","        ]\n","        conversations.append(conv)\n","\n","    # Process with Qwen processor\n","    texts = [\n","        processor.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n","        for conv in conversations\n","    ]\n","\n","    inputs = processor(\n","        text=texts,\n","        images=images,\n","        padding=True,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Add metadata\n","    inputs[\"image_ids\"] = [sample[\"image_id\"] for sample in batch]\n","    inputs[\"question_ids\"] = [sample[\"question_id\"] for sample in batch]\n","    inputs[\"questions\"] = questions\n","    inputs[\"answers\"] = [sample[\"answers\"] for sample in batch]\n","\n","    return inputs\n","\n","\n","def collate_fn_train_qwen(batch: List[Dict], processor, tokenizer) -> Dict:\n","    \"\"\"Collate function for Qwen2.5-VL model training with labels\"\"\"\n","    images = [sample[\"image\"] for sample in batch]\n","    questions = [sample[\"question\"] for sample in batch]\n","\n","    # Get most common answer for each sample\n","    dataset = TextVQADataset.__new__(TextVQADataset)\n","    answers = [dataset.get_most_common_answer(sample[\"answers\"]) for sample in batch]\n","\n","    # Create conversation format with answer for training\n","    conversations = []\n","    for q, a in zip(questions, answers):\n","        conv = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"image\"},\n","                    {\"type\": \"text\", \"text\": q}\n","                ]\n","            },\n","            {\n","                \"role\": \"assistant\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": a}\n","                ]\n","            }\n","        ]\n","        conversations.append(conv)\n","\n","    # Process with Qwen processor\n","    texts = [\n","        processor.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)\n","        for conv in conversations\n","    ]\n","\n","    inputs = processor(\n","        text=texts,\n","        images=images,\n","        padding=True,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Create labels (mask the input part, only train on output)\n","    labels = inputs[\"input_ids\"].clone()\n","\n","    # We need to mask everything except the answer tokens\n","    # For simplicity, we'll use the full sequence as labels\n","    # The model will learn to predict the assistant response\n","    inputs[\"labels\"] = labels\n","\n","    return inputs\n","\n","\n","def get_dataloader(\n","    data_dir: str,\n","    split: str,\n","    processor,\n","    batch_size: int = 4,\n","    shuffle: bool = True,\n","    num_workers: int = 4,\n","    max_samples: Optional[int] = None,\n","    for_training: bool = False,\n","    tokenizer = None,\n",") -> DataLoader:\n","    \"\"\"Create a DataLoader for the TextVQA dataset\"\"\"\n","\n","    dataset = TextVQADataset(\n","        data_dir=data_dir,\n","        split=split,\n","        processor=processor,\n","        max_samples=max_samples,\n","    )\n","\n","    if for_training:\n","        collate = lambda batch: collate_fn_train_qwen(batch, processor, tokenizer)\n","    else:\n","        collate = lambda batch: collate_fn_qwen(batch, processor)\n","\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        collate_fn=collate,\n","        pin_memory=True,\n","    )\n","\n","    return dataloader\n","\n","\n","if __name__ == \"__main__\":\n","    # Test the data loader\n","    data_dir = \"textvqa_data/data\"\n","\n","    dataset = TextVQADataset(data_dir, split=\"validation\", max_samples=5)\n","\n","    for i in range(min(3, len(dataset))):\n","        sample = dataset[i]\n","        print(f\"\\nSample {i}:\")\n","        print(f\"  Image ID: {sample['image_id']}\")\n","        print(f\"  Question: {sample['question']}\")\n","        print(f\"  Answers: {sample['answers'][:3]}...\")\n","        print(f\"  OCR Tokens: {sample['ocr_tokens']}\")\n","        print(f\"  Image size: {sample['image'].size}\")\n"]}]}